[
  {
    "path": "posts/gender_pay_disparity/",
    "title": "Gender pay disparity",
    "description": "Long tables are an inefficient way of reporting salary differences. Here I transform a census table into an reactive plot to convey the stark salary gap between professional men and women.",
    "author": [],
    "date": "2022-08-08",
    "categories": [],
    "contents": "\n\n\n\n\n\nBackground\nMy eyes tend to glaze over whenever a long table appears in general\nor scientific reporting. Here I attempt to convert a report on\ngender-based salary disparity into something more easily interpretable.\nThe problem will only hit home with the audience if the differences are\ndisplayed graphically.\nThe article (generously defined) in question is here.\nAim\nThe aim is to create an interactive graphic displaying the salary and\ngender pay gap for all listed professions.\n\nFirst let’s load some required packages\n\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(plotly)\n\n\nI’ve copied the men and women tables into text files. Read these\nusing read_tsv(), then use rename() to set the column names to occ\n(occupation) and sal (salary)\n\n\nmen <- read_tsv(\"project_data/men.txt\",  col_names = FALSE ) %>% rename(occ = X1, sal=X2) \n\nwm <- read_tsv(\"project_data/women.txt\", col_names = FALSE ) %>% rename(occ = X1, sal=X2) \n\n\nCombine the two tables side-by-side using full_join(), and append\n’_m’ and ’_w’ as the suffix to distinguish columns with the same\nname.\n\n\nsal_tbl <- full_join(men, wm, by='occ', suffix=c('_m','_w')) \n\n\nTry out the plot\n\n\nsal_tbl %>%  \n  pivot_longer(-occ) %>% \n  ggplot(aes(x=value, y=occ)) + \n  geom_point(aes(col=name, shape=name))\n\nWarning: Removed 16 rows containing missing values (geom_point).\n\n\nThere’s a warning that 16 data-points are missing. Check this by\ncounting the NA values in the men and women salary columns\n\n\nsal_tbl %>%  count(is.na(sal_m), is.na(sal_w))\n\n# A tibble: 3 × 3\n  `is.na(sal_m)` `is.na(sal_w)`     n\n  <lgl>          <lgl>          <int>\n1 FALSE          FALSE             42\n2 FALSE          TRUE               8\n3 TRUE           FALSE              8\n\nSo 8 data points are missing from each of the men’s and women’ list.\nThis indicates that the top 50 highest paid professions are not\ncompletely equivalent between the genders. The incomplete data will be\nmore obvious when the y axis is sorted (below).\nThe y axis labels are blowing the dimensions out, so let’s paraphrase\nthem using case_when() and str_detect(). Then we calculate the wage gap\nusing mutate()\n\n\nsal_tbl_edit <- sal_tbl %>% \n  mutate(occupation = case_when(\n    str_detect(occ, 'Executive/managing director') ~ 'Senior public servant',\n    str_detect(occ, 'Occupational medicine specialist') ~ 'Sports medicine physician',\n    TRUE ~ occ)) %>% \n  mutate(wage_gap = abs(sal_m - sal_w)) %>% \n  select(occupation, everything(), -occ)  \n\n\nNow to fix the order of the y axis (largest to smallest on men’s\nsalary); this is tricky but helped by this solution. When the\ny axis is discrete, the axis order will be built from the bottom up. In\nthe second step, we transform the shape of the data from wide to long,\nusing pivot_longer().\n\n\nsal_tbl_edit %>% arrange(!is.na(sal_m), sal_m, sal_w) %>% \n  mutate(occupation = factor(occupation, levels = occupation)) %>% \n  pivot_longer(cols = -c(occupation, wage_gap), names_to='gender', values_to='salary') %>% \n  ggplot(aes(x=salary, y=occupation)) + geom_point(aes(col=gender,shape=gender))\n\n\n\nNow we add a line between data for the same profession using\ngeom_line(). This geom is given before geom_point() so that the points\noverlie the line ends. We also use scale_shape_manual() to change the\nshape to assign women as circle and men as triangle, which is slightly\nmore intuitive given ♂ and ♀\nconventions. To check the encoding of different shapes, run\n?pch\n\n\nsal_tbl_edit %>% arrange(!is.na(sal_m), sal_m, sal_w) %>% \n  mutate(occupation = factor(occupation, levels = occupation)) %>% \n  pivot_longer(cols = -c(occupation,wage_gap), names_to='gender', values_to='salary') %>% \n  ggplot(aes(x=salary, y=occupation)) +  geom_line(aes(group=occupation)) +\n  geom_point(aes(col=gender, shape=gender),size=1.5) +\n  scale_shape_manual(values = c(17,19)) + theme_bw()\n\n\n\nIts good practice to separate the data (top lines) from the plotting\nfunction (bottom lines), and store the data as a new variable. In this\nstep we also divide the salary by 100000 and store as a new column\n(‘sal_slim’), to clean up the x axis numbers. When users scroll over the\ndatapoints however, we want the original number displayed.\n\n\nsal_tbl_final <- sal_tbl_edit %>% arrange(!is.na(sal_m), sal_m, sal_w) %>% \n  mutate(occupation = factor(occupation, levels = occupation)) %>%\n  pivot_longer(cols = -c(occupation, wage_gap),  names_to='gender', values_to='salary') %>% \n  mutate(sal_slim = salary/100000) \n\n\nFinally we can call the plot within plotly() add two features: a text\naesthetic and a tooltip, to make it interactive. We also add an x axis\nlabel, and delete the y axis label as the axis labels obviously relate\nto profession.\n\n\nplot_final <- sal_tbl_final  %>% \n  ggplot(aes(x=sal_slim, y=occupation)) + \n  geom_line(aes(group=occupation)) +\n  geom_point(aes( col=gender, shape = gender , \n                  text = paste0(occupation, ' $', salary, '\\n',\n                                'Wage gap:', '$', wage_gap) ), size=1.5) +\n  scale_shape_manual(values = c(17,19)) +\n  xlab('Salary ($100,000s)') + ylab('') +\n  ggtitle('Gender pay gap for most highly-paid professions', \n          subtitle='Australian census data') +\n  theme_bw()\n\n\nggplotly(plot_final , tooltip = 'text', width = 1000, height=800)\n\n\n\n It looks like the wage gap is generally larger when\nsalaries are higher. How does the relative wage gap change with greater\nabsolute men’s salary? To get an idea of the relationship between\nabsolute salary and gender pay gap, we can plot the men/women salary\nratio, and size the points by the absolute men’s salary.\n\n\nrel_gap_tbl <- sal_tbl_final %>%\n  select(1,gender,salary,wage_gap)  %>% \n  pivot_wider(names_from = 'gender',values_from='salary') %>% \n  mutate(ratio =  sal_m/sal_w) %>% \n  mutate(gender_col = ifelse(ratio>1,'Men','Women')) \n  \nrel_gap_plot <- rel_gap_tbl %>% na.omit() %>%  \n  ggplot(aes(y=occupation)) + \n  geom_point(aes(x=ratio,col=gender_col, size=wage_gap, \n                 text=paste0(occupation,'\\n', \n                             'Wage gap: ', '$', wage_gap))) + \n  geom_vline(xintercept = 1, lty=2) +\n  scale_shape_manual(values = c(17,19)) +\n  xlab(\"Men's pay ÷ women's pay\" ) + ylab('') +\n  ggtitle('Relative gender pay gap for highly-paid professions', \n          subtitle='Australian census data') +\n  theme_bw() \n  \nggplotly(rel_gap_plot , tooltip = 'text', width = 1000, height=800)\n\n\n\n\n\n\nConclusions\nThe differences are stark when the data are presented graphically. In\nterms of improvements to the charts, for the first chart the data is\nincomplete, as only the top 50 salaried professions were included for\neach gender. The salary for metallurgist men may be greater than for\nwomen, but this profession ranked in the top 50 only for women, and as\nsuch the data for men is missing. In addition, the salaries for women sports physicians is likely\nless than $100,000. To better display this a line from 1 to 1.875 could\nbe added, indicating the presence of a datapoint < 100000. Even\nbetter would be curating a complete list of data from the original\ncensus link! Lastly, the legend labels are cryptic and could be updated\nto ‘Men’ and ‘Women’.\nPlotly doesn’t parse all of the ggplot commands (for example\nggtitle(subtitle) ) and produces some interesting legend notation.\n Endless refinement is possible, but at least now the take-home\nmessages are clear:Even in high-paid professions, higher salaries are associated\nwith higher gender pay disparity, and the medical profession is\nparticularly fraught.\n\n\n\n",
    "preview": "posts/gender_pay_disparity/gender-pay-disparity_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2022-08-08T22:59:35+10:00",
    "input_file": {}
  },
  {
    "path": "posts/journal_IF_vs_cost/",
    "title": "Journal fees vs impact",
    "description": "Journal impact factor vs publication fees.",
    "author": [],
    "date": "2022-08-08",
    "categories": [],
    "contents": "\n\n\n\nWhen a research manuscript is finally ready for submission, its time\nfor the delicate discussion of journal publication fees. Publishing\n‘open-access’ (i.e., free for the public to read) attracts a higher\ncharge than pay-per-view. In either case, the public, who in many cases\npaid for the research, also ultimately pay\nto read the results.\nThe strong influence that corporate publishing houses have on\nscience, especially in biology, is due to their prestigious reputations.\nHighly read and highly cited journals are considered ‘high-impact’, and\nassigned an ‘impact factor’ (something approximating average number of\ncitations per publication in the last year). This number is often used\nto judge the quality of a manuscript appearing in a journal, and affects\nthe careers of its authors.\nThe impact factor system is flawed for many reasons that others have\neloquently articulated,\nbut given that the culture of impact factor-driven science still\nabounds, I wanted to compare the range of impact bang available for the\npublic buck.\nHere I investigate the relationship between impact factor and\npublication fees for a sample of Elsevier biological journals, using the\nweb scraping and parallel processing functionality of R. ‘Scraping’ is\nthe harvesting of data from websites by searching through the underlying\nhtml code to extract text.\nRead remote-hosted data\nFirst we use the rio() library to import an excel file\nof journal publication\nfees hosted at Elsevier.\n\n\nlibrary(tidyverse)\n\n#thanks to https://stackoverflow.com/a/63910298\n\nurl <- \"https://zenodo.org/record/1143853/files/elsevier_j.custom97.xlsx\"\n\noa_cost <- rio::import(file= url) %>% select(1:5) %>% glimpse()\n\nRows: 2,761\nColumns: 5\n$ ISSN            <chr> \"1876-2859\", \"1076-6332\", \"0001-4575\", \"0155…\n$ `Journal Title` <chr> \"Academic Pediatrics\", \"Academic Radiology\",…\n$ `OA Model`      <chr> \"Hybrid\", \"Hybrid\", \"Hybrid\", \"Hybrid\", \"Hyb…\n$ Currency        <chr> \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"USD\", \"U…\n$ Price           <chr> \"3000\", \"2750\", \"3550\", \"1100\", \"1800\", \"275…\n\nnames(oa_cost) <- c('isbn','title','model','curr','fee')\n\n\nThere are some journals with author fee exemptions which are\nirrelevant for the question at hand and are removed\n\n\noa_cost %>% filter(str_detect(title,'\\r\\n† fee not payable by author')) %>% select(title) %>% head()\n\n                                                                                  title\n1                            Acta Pharmaceutica Sinica B\\r\\n† fee not payable by author\n2                                       Acta Sociológica\\r\\n† fee not payable by author\n3                  African Journal of Emergency Medicine\\r\\n† fee not payable by author\n4                             African Journal of Urology\\r\\n† fee not payable by author\n5                          Ain Shams Engineering Journal\\r\\n† fee not payable by author\n6 AKCE International Journal of Graphs and Combinatorics\\r\\n† fee not payable by author\n\noa_cost <- oa_cost %>% mutate(title = str_remove(title,'\\r\\n† fee not payable by author'))\n\n\nNow to convert the journal titles to lower case, and select some\nbiology journals using a soft match on ‘bio’,‘gene’,‘geno’ and\n‘life’:\n\n\noa_cost_biol <- oa_cost %>% mutate(title_lower = tolower(title)) %>% \n  filter(str_detect(title_lower,'bio|geno|gene|life'))  \n\n\nScrape sciencedirect\nTo scrape the impact factor for each journal from the ScienceDirect\ndatabase requires dynamically constructing the url, based on a template\nsuch as https://www.sciencedirect.com/journal/neurobiology-of-stress.\nThe journal title will be lower case and hyphen-separated, appended\nafter the final “/”. Based on trial and error, the string manipulation\nbelow to clean up commas, replace spaces with hyphens, and handle some\nedge cases, results in valid urls.\n\n\noa_for_scidir <- oa_for_url %>% \n  mutate(\n  title_lower = str_replace_all(title_lower,' ','-'),\n  title_lower = str_replace_all(title_lower,',',''),\n  title_lower = str_replace_all(title_lower,'bba---','biochimica-et-biophysica-acta-bba-'),\n  title_lower = str_replace_all(title_lower,'&','and'),\n  title_lower = str_remove_all(title_lower,':')) %>%     \n  filter(!str_detect(title_lower,'-part-|biologies'))\n\n\n ## Workshop rvest rvest is the main web scraping\npackage in the tidyverse suite. It has handy functions to extract\nfeatures from html, such as html_text() and\nhtml_table(). Searching the desired information in the\nwebsite source code will help to identify the relevant html nodes. This\ncan be tricky business but there is help\navailable!\nThe solution below isn’t elegant but gets the job done for a test\nexample.\n\n\nlibrary(rvest)\n\nx <- \"neurobiology-of-stress\"\n\nread_html(paste0(\"https://www.sciencedirect.com/journal/\", x)) %>% \n  html_nodes('span') %>% html_text() %>% \n  as_tibble() %>%\n  filter(str_detect(value,'Impact')) %>% \n  #retain only digits\n  filter(str_detect(value,'\\\\d')) %>% \n  #add a column containing the journal name, stored in x \n  mutate(journal=x)\n\n# A tibble: 1 × 2\n  value              journal               \n  <chr>              <chr>                 \n1 7.142Impact Factor neurobiology-of-stress\n\nThis code is now assigned as a function called sd_scrape, and wrapped\nwith tryCatch(), to make sure any errors in hitting the\nwebsite are logged, rather than causing the job to fail completely:\n\n\nsd_scrape <- function(x){\n  \n  tryCatch(\n  #web scraping code\n  read_html(paste0(\"https://www.sciencedirect.com/journal/\", x)) %>% \n      html_nodes('span') %>% html_text() %>% as_tibble() %>%\n      filter(str_detect(value,'Impact')) %>% \n      filter(str_detect(value,'\\\\d')) %>% \n      mutate(journal=x), \n  #if an individual site scrape fails, create a 1-row results table with 'error' and the journal name \n  error = function(c) tribble(~value, ~journal, \"error\", x)\n    )\n}\n\n\nParallelize with furrr\nThe next code block does a lot of heavy lifting, and will ideally\nonly run once. At the outset is an if statement to make sure the rest of\nthe code block runs only if the rvest output is missing. The\nscraping function is run using furrr::future_map(), which\nmakes it run in parallel — ideally 4x faster than scraping each journal\nwebsite one by one. I borrow heavily from this furrr\ntutorial.\n\n\nlibrary(furrr)\n\nif(exists(\"oa_impact\")){print('oa_impact exists!')}else{\n\n# set up parallel processors\nfuture::plan(multisession(workers = 4)) # parallelly::availableCores()\n\n# scrape sciencedirect based on an input vector of journal titles, and store results in oa_impact\noa_impact <- future_map( oa_for_scidir$title_lower , sd_scrape) %>% \n  bind_rows() %>% \n  #remove text from value column\n  mutate(value = as.numeric(str_remove(value,'Impact Factor')))\n}\n\n\nJoin tables\nJoin the scraped impact factor and publication fee tables, and tag\nselected journals for increased plotting point size.\n\n\noa_cost_impact <- oa_impact %>% \n  left_join(oa_for_scidir, by = c('journal' = 'title_lower')) %>% \n  mutate(fee = as.numeric(fee)) %>% \n  #tag selected journals for increased plotting point size\n  mutate(upsize = ifelse(journal %in% c('redox-biology','bioactive-materials',\n                                        'international-journal-of-food-microbiology',\n                                        'trends-in-cell-biology','current-biology'), 'y','n') )\n\n\nPlot the results\nThis code fits a quadratic curve to the data, as well as jittering\nthe points by $100 to handle over-plotting around x = $3000. The text\naesthetic is not relevant for static ggplots, but required for\ninteractive plotting below.\n\n\nplot_scrape <- oa_cost_impact %>% \n    ggplot(aes(x = fee, y = value)) + \n    geom_point(data = . %>% filter(upsize=='n'), \n               aes(col = fee, text = title, shape = model), \n             position = position_jitterdodge(jitter.width = 100, jitter.height  =  0),\n             alpha = 0.6) +\n  scale_color_gradient(low = 'blue', high = 'red') +\n  #ggrepel::geom_text_repel(data = . %>% filter(upsize=='y'), \n  #                         mapping = aes(label=title, col=fee)) +\n  geom_point(data = . %>% filter(upsize=='y'), \n              aes(col = fee, text = title, shape = model), size=3) +\n  stat_smooth(method  =  \"lm\", formula  =  y ~ x + I(x^2), \n              size = 1, se =  F, col = \"dark blue\" ) + #fits parabola\n  xlab(\"cost USD$\") + ylab(\"Impact Factor\") +\n  ggtitle(\"Publication Fee vs Impact Factor\") +\n  guides(shape = guide_legend(title = \"\")) # handles plotly legend overlap \n\nplot_scrape \n\n\n\nInteractive plots\nThe plotly() package allows conversion of static ggplots\ninto interactive plots. The key is to include the tooltip, which will\nlink to the text aesthetic in the ggplot call, and render the relevant\ntext when the user hovers the mouse over a data point.\nggplotly() renders legends slightly differently and can\nlead to crowding. This is handled with the plotly::layout()\nfunction.\n\n\n\n\n\n\nConclusions\nThe quadratic curve gives a reasonable fit and incorporates the\npoints at x = $0. There is a large spread of impact factors around the\nline, meaning that the publication fee is a poor proxy for impact\nfactor, and vice versa. Despite this, journals with higher impact\nfactors tend to charge higher publication fees, and there are a good\nnumber of exceptionally expensive lower impact journals at the bottom\nright of the plot. Some extreme data points have been enlarged as plotly\nstruggles with labels.In so far as value can be measured in this context, better to be\na bioactive materials or redox researcher than a food\nmicrobiologist!\n\n\n\n",
    "preview": "posts/journal_IF_vs_cost/IF_by_cost_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2022-08-08T23:11:41+10:00",
    "input_file": {}
  },
  {
    "path": "posts/shinkansen/",
    "title": "Shinkansen",
    "description": "To animate some trains!",
    "author": [],
    "date": "2022-08-08",
    "categories": [],
    "contents": "\nThe better kind of bullets\n\n\n\nAfter returning from Japan feeling bereft of high-tech public\ntransport options, I started a project that would chart the movement of\nshinkansen (bullet trains) across Japan in near real time. Whereas sites\nlike tetsudonow display\nexcellent real-time interactive metro line service animations, I wanted\nto see trains running over a larger spatial scale. Real time\ninteractivity is a longer-term goal.\nStarting from a single column of station names and stop times, this\nturned out to be a tricky undertaking, involving date/time manipulation,\nworld map data, and rendering non-standard text and emojis wrapped as\nanimations.\nBelow, instead of walking through every step, I outline the main\nproblems I encountered and my solutions. Anyone interested in the\ngrizzly details can read my comments in the code chunks.\nThe project required four main components:\nscraping real timetable data from jorudan.co.jp,\nmapping latitude longitude and elevation data to\nrecreate the train route and surrounding topography,\nanimating train icons according to the timetable\ndata, and\ncalculating the unpublished passage times for\nexpress trains through non-stopping stations.\nScraping timetable data\nThe selected timetable is for the Tokyo-Nagano-Kanazawa (‘Hokuriku’)\nline, during the time window 1-7pm on 22 March 2020. There are six\nservices departing Tokyo on this timetable, labelled service 3-8. Using\nthe rvest library we read the table data from the timetable url.\n\n\nlibrary(rvest)\nlibrary(lubridate)\nlibrary(tidyverse)\n\n\n\n\nl1 <- list.files('project_data/', pattern='Rds')\n\nif('raw_rvest_data.Rds' %in% l1){\n  p1 <- readRDS('project_data/raw_rvest_data.Rds')\n  print('Reading from local file to avoid \"digital rot\"!')\n  }else{\n    \nurl <- \"https://www.jorudan.co.jp/time/cgi/time.cgi?Csg = 0&Sok = 1&pg = 21&rf = tm&lnm = %A4%A2%A4%B5%A4%DE613%B9%E6%28E7%2FW7%B7%CF%29%28%C4%B9%CC%EE%B9%D4%29&rnm = %CB%CC%CE%A6%BF%B7%B4%B4%C0%FE&eki1 = %C5%EC%B5%FE&eki2 = D%B6%E2%C2%F4&Dym = 202203&Ddd = 14&Dhh = 14&Dmn = 24&Dw = 0\"\n\np1 <- read_html(url) %>% \n  html_nodes('td') %>% html_text() %>% \n  as_tibble() %>% mutate(value = str_remove_all(value,  \"\\n\"))\n  \nsaveRDS(p1, file='project_data/raw_rvest_data.Rds')\n\n}\n\n[1] \"Reading from local file to avoid \\\"digital rot\\\"!\"\n\nprint(p1,n=20)\n\n# A tibble: 162 × 1\n   value           \n   <chr>           \n 1 \"■\"             \n 2 \"東京\"          \n 3 \" 13:04発\"      \n 4 \" 13:24発\"      \n 5 \" 14:04発\"      \n 6 \" 14:24発\"      \n 7 \" 15:04発\"      \n 8 \" 15:24発\"      \n 9 \"\"              \n10 \"■\"             \n11 \"上野\"          \n12 \"13:09着13:10発\"\n13 \"13:29着13:30発\"\n14 \"14:09着14:10発\"\n15 \"14:29着14:30発\"\n16 \"15:09着15:10発\"\n17 \"15:29着15:30発\"\n18 \"\"              \n19 \"■\"             \n20 \"大宮\"          \n# … with 142 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\nThe scraped data is a single vector in repeating units of 9 rows. To\nwrangle this into a timetable we force it into a matrix of 9 columns.\nThis gives every second station. The interspersed stations are in rows\n10-18. To create the final timetable we assign odd and even row numbers\nthen interleave the two tables:\n\n\njikoku <- rbind(\n  \n  #Table 1 odd number stops\n  as_tibble(t(matrix(p1$value, ncol  =  9))) %>% \n  as_tibble() %>% \n  select(V1:V9) %>% \n  mutate(order  =  seq(1,  2*(nrow(.)),  2)),\n \n  #Table 2 even number stops \n  as_tibble(t(matrix(p1$value, ncol  =  9))) %>% \n    as_tibble() %>% \n    select(V10:V18) %>% \n    mutate(order  =  seq(2,  2*(nrow(.)),  2)) %>% \n    magrittr::set_colnames(c(paste0('V', 1:9), 'order')) \n) %>% \n  #Arrange by row number\n  arrange(order) %>% \n  select(V2, order, everything(), -V1, -V9) %>% rename(Station = V2) %>% \n  print()\n\n# A tibble: 18 × 8\n   Station        order V3             V4      V5    V6    V7    V8   \n   <chr>          <dbl> <chr>          <chr>   <chr> <chr> <chr> <chr>\n 1 東京               1  13:04発        13:24…  14:…  14:…  15:…  15:…\n 2 上野               2 13:09着13:10発 13:29…  14:0… 14:2… 15:0… 15:2…\n 3 大宮               3 13:28着13:29発 13:48…  14:2… 14:4… 15:2… 15:4…\n 4 熊谷               4 13:41着13:42発 ↓       14:4… ↓     15:4… ↓    \n 5 本庄早稲田         5 13:50着13:51発 ↓       14:5… ↓     15:5… ↓    \n 6 高崎               6 14:00着14:01発 14:14…  15:0… 15:1… 16:0… 16:1…\n 7 安中榛名           7 ↓              ↓       15:0… ↓     ↓     ↓    \n 8 軽井沢             8 14:16着14:17発 ↓       15:2… ↓     16:1… 16:2…\n 9 佐久平             9 14:25着14:26発 ↓       15:3… ↓     16:2… ↓    \n10 上田              10 14:35着14:36発 ↓       15:3… ↓     16:3… ↓    \n11 長野              11 14:47着        14:51…  15:5… 15:5… 16:4… 16:5…\n12 飯山              12                15:04…        ↓           17:0…\n13 上越妙高          13                15:15…        16:1…       17:1…\n14 糸魚川            14                15:28…        16:2…       17:3…\n15 黒部宇奈月温泉    15                15:43…        16:4…       17:4…\n16 富山              16                15:56…        16:5…       17:5…\n17 新高岡            17                16:05…        17:0…       18:0…\n18 金沢              18                16:19…        17:1…       18:2…\n\nNow we reshape the timetable into long format, and separate the times\nfor station arrival (着) and departure (発) into two columns, before\nspreading the data back into a readable timetable with each service as a\nnew column. The express (non-stopping) stations are now encoded as NA\ninstead of “↓”.\n\n\njikoku_clean <- jikoku %>% gather(key, value, -c(Station, order)) %>% \n  mutate(key = str_replace(key, 'V', 'service_')) %>% \n  rename(service = key) %>% \n  mutate(value = str_replace(value, \"↓\", NA_character_)) %>% \n  separate(value,  into = c('arr', 'dpt'), sep = \"着\") %>% \n  #move into the dpt column any data in arrival column containing departure information ('発'):\n  mutate(dpt  = ifelse(str_detect(arr, '発'), arr, dpt)) %>% \n  mutate(arr = ifelse(str_detect(arr, '発'),  NA, arr)) %>% \n  #drop the kanji:\n  mutate(dpt  =  str_remove_all(dpt, '発')) %>% \n  #create a longer-format table to separate arr and dpt rows:\n  gather(key, value, arr, dpt) %>% \n  arrange(order, Station, value) %>% \n  na.omit() %>% \n  #create a wide-format table with service ('service') as new column names:\n  spread(service, value) %>% arrange(order) \n\n#NB because Japanese font doesn't seem to render on interactive tables, for ease of viewing I drop Station column from output:\njikoku_clean %>% select(-Station)\n\n# A tibble: 35 × 8\n   order key   service_3 service_4 service_5 service_6 servi…¹ servi…²\n   <dbl> <chr> <chr>     <chr>     <chr>     <chr>     <chr>   <chr>  \n 1     1 dpt    13:04     13:24     14:04     14:24     15:04   15:24 \n 2     2 arr   13:09     13:29     14:09     14:29     15:09   15:29  \n 3     2 dpt   13:10     13:30     14:10     14:30     15:10   15:30  \n 4     3 arr   13:28     13:48     14:28     14:48     15:28   15:48  \n 5     3 dpt   13:29     13:49     14:29     14:49     15:29   15:49  \n 6     4 arr   13:41     <NA>      14:41     <NA>      15:41   <NA>   \n 7     4 dpt   13:42     <NA>      14:42     <NA>      15:42   <NA>   \n 8     5 arr   13:50     <NA>      14:50     <NA>      15:50   <NA>   \n 9     5 dpt   13:51     <NA>      14:51     <NA>      15:51   <NA>   \n10     6 arr   14:00     14:14     15:00     15:14     16:00   16:12  \n# … with 25 more rows, and abbreviated variable names ¹​service_7,\n#   ²​service_8\n# ℹ Use `print(n = ...)` to see more rows\n\nGeographic maps\nThis step requires the plotting coordinates for Honshu island, and\nGPS coordinates and English translations for the Hokuriku line\nstations.\nPlot the land mass\nUsing the maps library we can extract coordinates for Japan, then use\nggplot with coord_quickmap() to flatten the latitude and\nlongitude into 2D space. Somehow colouring the plot background blue\n(‘theme_sea’) requires about as much code as does the much more\ncomplicated mapping function….\n\n\nlibrary(maps)\nJPAN <- filter(map_data(\"world\"), region==\"Japan\") %>% as_tibble()\n\njp_map <- ggplot(JPAN, aes(x = long, y = lat, group = group)) +\n  geom_polygon(fill = \"white\", colour = \"black\") +\n  coord_quickmap()\n\ntheme_sea <- theme_classic() + theme(panel.background = element_rect(fill=\"light blue\"),\n                                     legend.position = 'bottom') # <- required for later\n\njp_map + theme_sea\n\n\n\nCity GPS points\nNext we need the google maps API to search the latitude and longitude\nof each city on the line. This can be costly for big projects. Anyone\ntrying to recreate this project can use the supplied data (github)\ninstead of running the geocode() commands.\nFor this step I borrowed heavily from this blog post: https://www.littlemissdata.com/blog/maps and this SO\nthread.\n\n\n#Read table of translated station names\nhokuriku <- read_csv('project_data/hokuriku_translated.csv', show_col_types = F, col_names  =  TRUE)\n\n\n\n\n#devtools::install_github(\"dkahle/ggmap\",  ref  =  \"tidyup\",  force = TRUE)\n\n#Load the library\nlibrary(\"ggmap\")\n\n#Set the Google Maps API\n#  httr::set_config(httr::config(http_version = 0))\n\n#Set your API Key \nregister_google(key  =  \"<<your key here>>\")\n\ngeocode('Tokyo,  Japan')\n\n\nNow to save some money:\n\n\nll_files <- list.files( 'project_data/', pattern='j_train_lat_lon_GCPbill')\n\nif('j_train_lat_lon_GCPbill.tsv' %in% ll_files){\n  print('lat_long exists, dont rerun GCP billing')\n  \n  jikoku_gps <- read_tsv('project_data/j_train_lat_lon_GCPbill.tsv') \n  jikoku_gps %>% select(1,English,lon,lat) %>% print()\n  \n  }else{\n    \njikoku_gps <- jikoku_clean %>% \n  left_join(hokuriku, by = c('Station' = 'Japanese')) %>% \n  mutate(English  =  paste0('JR ', English,  ' Station,  Japan')) %>% \n  #muate_geocode() searches for GPS coordinates in the supplied column of location names: \n  mutate_geocode(location = English) %>% \n  group_by(Station,English) %>% fill(lon,lat, .direction = 'down') %>% ungroup()\n\n#manual repair mis-translated: Sakudaira & Annakaharuna\n#manual repair Karuizawa: 36.34325739599876, 138.63521437877966\n\njikoku_gps <- jikoku_gps %>% \n  mutate(lon = ifelse(English=='JR Karuizawa Station,  Japan', 138.63521437877966, lon),\n         lat = ifelse(English=='JR Karuizawa Station,  Japan', 36.34325739599876,  lat)) %>% \n  #strip redundant text:\n  mutate(English=str_remove_all(English,'JR '),\n         English=str_remove_all(English,' Station,  Japan'))\n  \nwrite_tsv(jikoku_gps, 'project_data/j_train_lat_lon_GCPbill.tsv')\n}\n\n[1] \"lat_long exists, dont rerun GCP billing\"\n# A tibble: 35 × 4\n   Station    English        lon   lat\n   <chr>      <chr>        <dbl> <dbl>\n 1 東京       Tokyo         140.  35.7\n 2 上野       Ueno          140.  35.7\n 3 上野       Ueno          140.  35.7\n 4 大宮       Omiya         140.  35.9\n 5 大宮       Omiya         140.  35.9\n 6 熊谷       Kumagaya      139.  36.1\n 7 熊谷       Kumagaya      139.  36.1\n 8 本庄早稲田 Honjo-Waseda  139.  36.2\n 9 本庄早稲田 Honjo-Waseda  139.  36.2\n10 高崎       Takasaki      139.  36.3\n# … with 25 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\nWe can now overlay the stations on the Honshu map and zoom in on the\nline:\n\n\nhokuriku_map <- jikoku_gps %>% select(order, Station,English,lon,lat) %>% distinct() %>% \n  ggplot2::ggplot(aes(x=lon,y=lat, group=order)) +\n  geom_polygon( data=JPAN, aes(x = long, y = lat, group = group), \n                fill = \"white\", colour ='black') +\n  geom_path(group=1, lty=2, col=\"black\", lwd=0.5) +\n  ggrepel::geom_text_repel( aes(label=English, y=lat, col= order), nudge_y = 0.1, cex=3, \n                            seed=1234,show.legend = F) +  \n  geom_point( aes(col= order), cex=3, pch=21, bg='white', show.legend = F) + \n  scale_color_viridis_c(end=0.65, direction = -1) + \n  ylab('Latitude') + xlab('Longitude') +\n  theme_sea +\n  coord_quickmap(xlim=c(136, 140.5), ylim=c(35.15,37.45))\n  \nhokuriku_map\n\n\n\nWhile this is a fine result, there is a lot of empty space that could\nbe made more interesting by including Japan’s mountainous terrain- a key\nelement in the scenic excitement of bullet train rides. To do this\nrequired a coding odyssey that might be the subject of another post. We\nwill use the resulting elevation .png file as a background for the plot\ndata, in place of the geom_polygon geographic outline used above.\n\n\nlibrary(magick); library(png); library(grid); library(cowplot)\n\nmypng <- readPNG('project_charts/raster_hills_japan_latlon.png')\njpn_png_grob <- rasterGrob(mypng)\n \nhokuriku_map_elev <- jikoku_gps %>% select(order, Station,English,lon,lat) %>% distinct() %>% \n  ggplot(aes(x=lon,y=lat, group=order)) +\n  #render the elevation png image as the first layer in the plotting space\n  draw_image(mypng, interpolate = T, x = 135.75,y=34.55, width=5, height=3.5) + \n  geom_path(group=1, lty=2, col=\"black\", lwd=0.5, alpha=0.5) +\n  ggrepel::geom_text_repel( aes(label=English, y=lat, col= order), nudge_y = 0.1, cex=2, \n                            seed=1234,show.legend = F) +  \n  geom_point( aes(col= order), cex=3, pch=21, bg='white', show.legend = F) + \n  scale_color_viridis_c(end=0.65, direction = -1) + \n  ylab('Latitude') + xlab('Longitude') +\n  coord_quickmap(xlim=c(136, 140.5), ylim=c(35.15,37.45))\n  \nhokuriku_map_elev\n\n\n\nCalculate inter-stop\ndistances\nLastly for the mapping stage, in order to interpolate the express\nstation pass-thru times (below), we need to calculate the distance (kM)\nbetween each pair of consecutive stops, and the cumulative line\ndistance. The geosphere library allows Euclidean distance calculations\n(given the lat-lon points are on a sphere).\n\n\nlibrary(geosphere)\n\n#calculate pairwise distances\n\npws_dist <- jikoku_gps %>% select(Station, English, lat, lon) %>% distinct() %>% \n  mutate(lag_lon  =  lag(lon),  lag_lat = lag(lat)) %>%\n  rowwise() %>% \n  mutate(stopDist = 0.001 * distHaversine(c(lon,  lat),  c(lag_lon,  lag_lat))) %>% \n  ungroup() %>% \n  mutate(across(.cols = c(contains('lag'),stopDist), ~replace_na(.x, 0))) %>% \n  mutate(cumDist =  cumsum(stopDist)) %>% \n  print()\n\n# A tibble: 18 × 8\n   Station        English    lat   lon lag_lon lag_lat stopD…¹ cumDist\n   <chr>          <chr>    <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 東京           Tokyo     35.7  140.      0      0      0       0   \n 2 上野           Ueno      35.7  140.    140.    35.7    3.78    3.78\n 3 大宮           Omiya     35.9  140.    140.    35.7   25.5    29.3 \n 4 熊谷           Kumagaya  36.1  139.    140.    35.9   33.4    62.7 \n 5 本庄早稲田     Honjo-W…  36.2  139.    139.    36.1   20.9    83.5 \n 6 高崎           Takasaki  36.3  139.    139.    36.2   18.9   102.  \n 7 安中榛名       Annakah…  36.4  139.    139.    36.3   15.3   118.  \n 8 軽井沢         Karuiza…  36.3  139.    139.    36.4   19.3   137.  \n 9 佐久平         Sakudai…  36.3  138.    139.    36.3   17.0   154.  \n10 上田           Ueda      36.4  138.    138.    36.3   23.3   177.  \n11 長野           Nagano    36.6  138.    138.    36.4   28.0   205.  \n12 飯山           Iiyama    36.8  138.    138.    36.6   27.2   233.  \n13 上越妙高       Joetsu-…  37.1  138.    138.    36.8   28.0   261.  \n14 糸魚川         Itoigawa  37.0  138.    138.    37.1   34.6   295.  \n15 黒部宇奈月温泉 Kurobe-…  36.8  138.    138.    37.0   35.4   331.  \n16 富山           Toyama    36.7  137.    138.    36.8   35.4   366.  \n17 新高岡         Shintak…  36.7  137.    137.    36.7   18.1   384.  \n18 金沢           Kanazawa  36.6  137.    137.    36.7   36.5   421.  \n# … with abbreviated variable name ¹​stopDist\n\nHow to train your animation\nThe gganimate\npackage is a neat addition to the ggplot vocabulary. The\ntransition_ geom family allows animation of data (iterative\npng snap-shots) in the specified column. Given each frame is rendered\nfrom a separate plotting call, any geoms with random jittering\n(e.g. geom_jitter, ggrepel) will jump around. This is fixed by\nspecifying seed in the map call above. For this prototyping\nstage, we use the ‘blank’ geographical background (no elevation) which\nis faster to render and creates a much smaller gif.\n\n\nlibrary(gganimate)\n\ntest_service <- jikoku_gps %>% select(-c(service_4:service_8)) %>% \n  #omit NA and empty cells:\n  filter(str_detect(service_3,':')) %>% \n  mutate(service_3_dttm = as_datetime(hm(service_3) ))\n\nani_plt <- hokuriku_map + \n  geom_point(data = test_service, aes(x=lon, y=lat, group = 1), col='red') +\n  transition_reveal(service_3_dttm )\n\nanimate(ani_plt, height = 4, width = 6, units = \"in\", res = 150, duration=10,nframes=100)\n\n\n\nNow to add the all-important bullet train emoji using emoGG:\n\n\n#devtools::install_github('dill/emoGG')\nlibrary(emoGG)\nani_plt_train <- hokuriku_map + \n  geom_emoji( data = test_service, aes(x=lon, y=lat, group = 1), cex=0.04, \n              #emoji='1f685') +  \n              emoji='1f686') +  \n  transition_reveal(service_3_dttm )\n\n\nanimate(ani_plt_train, height = 4, width = 6, units = \"in\", res = 150, duration=10,nframes=100)\n\n\n\nYou might have noticed that Annakaharuna station is skipped by\nService 3. This slightly annoying crack in the animation becomes a\ngaping chasm when super-/express Hakutaka & Kagayaki services are\nadded to the plotting data. They simply don’t follow the track through\nthe skipped stations. To remedy this required a lot of head-scratching\nand some Google persistence.\nExpress to insanity\nThe timetable contains NA values for skipped stations. For services\nthat terminate at mid-point stations, e.g. Nagano, subsequent station\ntimes contain blank text, not NA.\nThis problem was solved in multiple steps:\n1. Retain only the stations between start and terminal stations for each\nservice\n2. For express services, calculate time, distance, and thereby speed\nbetween stops (i.e., stations where the service actually stops)\n3. Join the shorter express data table to the full station list, and use\naverage speed and distance to interpolate the clock time when the train\nwill pass through non-stopping stations.\n4. Plot the interpolated data so that each service follows the track\nthrough all stations.\n1. Tag the serviced stations\nFirst join the timetable and station GPS coordinates, avoid distance\nredundancy & recode time\n\n\n# Join timetable with station gps coordinates, avoid distance redundancy & recode time #\n\nserviced_stn_time <- jikoku_gps %>% left_join(pws_dist %>% select(-starts_with('lag'))) %>% \n  #avoid duplicating stopDist and cumDist by replacing the departure row values with 0:   \n  mutate(stopDist = ifelse(key == 'arr', stopDist, 0)) %>% \n  mutate(cumDist  = ifelse(key == 'arr', cumDist,  0)) %>% \n  select(1:key,lon,lat, cumDist,everything()) %>% \n  #convert to long-form data:\n  gather(key = service ,value = time, starts_with('service_'), na.rm = F) %>% \n  select(order,Station,English,key,lon:lat,stopDist,cumDist,everything()) %>% \n  #recode time (character) to datetime format:\n  mutate(time_recode = as_datetime(hm(time))) \n\n\nNext, filter long format table to retain only stations en route for\neach service:\n\n\nserviced_stn_long <- serviced_stn_time %>%   \n    #First, select only stations within the service timespan, and fill up \\\n    #  (i.e., omitting stops after mid-point termini for half-services).\n  group_by(service) %>% arrange(service,order) %>% #this retains order (NA times for skipped stations are otherwise bumped to bottom)\n  mutate(route_stn = ifelse(\n        time_recode == min(na.omit(time_recode))|time_recode == max(na.omit(time_recode)), \n        1, 0)) %>% \n    #Next fill the route stations only:\n  fill(route_stn, .direction = 'up') %>% \n  mutate(route_stn = ifelse(!is.na(route_stn),'y','n')) %>% ungroup() %>% \n  filter(route_stn!='n') %>% \n  #Drop the tags now the filter is complete:\n  select(-contains('stn')) %>% print()\n\n# A tibble: 162 × 11\n   order Station    English  key     lon   lat stopD…¹ cumDist service\n   <dbl> <chr>      <chr>    <chr> <dbl> <dbl>   <dbl>   <dbl> <chr>  \n 1     1 東京       Tokyo    dpt    140.  35.7    0       0    servic…\n 2     2 上野       Ueno     arr    140.  35.7    3.78    3.78 servic…\n 3     2 上野       Ueno     dpt    140.  35.7    0       0    servic…\n 4     3 大宮       Omiya    arr    140.  35.9   25.5    29.3  servic…\n 5     3 大宮       Omiya    dpt    140.  35.9    0       0    servic…\n 6     4 熊谷       Kumagaya arr    139.  36.1   33.4    62.7  servic…\n 7     4 熊谷       Kumagaya dpt    139.  36.1    0       0    servic…\n 8     5 本庄早稲田 Honjo-W… arr    139.  36.2   20.9    83.5  servic…\n 9     5 本庄早稲田 Honjo-W… dpt    139.  36.2    0       0    servic…\n10     6 高崎       Takasaki arr    139.  36.3   18.9   102.   servic…\n# … with 152 more rows, 2 more variables: time <chr>,\n#   time_recode <dttm>, and abbreviated variable name ¹​stopDist\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n2.\nCalculate inter-stop clock time, distance and speed for express\nservices\n\n\n# Inter-stop time, distance and speed #\n\nstopDist_Dur_Speed_xprss <- serviced_stn_long %>% \n  #skip non-stopping stations:\n  filter(!is.na(time)) %>% \n  #calculate distance between stopping stations (lag n = 2 allows access to cumDist value in the departure rows):\n  group_by(service) %>% arrange(service,order) %>% \n  mutate(stopDist_xprss = cumDist - lag(cumDist, n = 2)) %>% \n  #update NA vals with inter-stop distance for the first rows (i.e., missed by lag(n=2)):\n  mutate(stopDist_xprss = ifelse(is.na(stopDist_xprss), stopDist, stopDist_xprss)) %>% \n  select(1:cumDist, stopDist_xprss, everything()) %>% \n  #create time_lag between stopping stations:\n  mutate(time_lag = lag(time_recode)) %>% \n  #calculate time duration between stopping stations:\n  mutate(dur  =  time_recode - time_lag) %>% \n  #impose a 30 second duration for stations where the stop is < 1 minute (i.e., arr time == dep time but NOT a skipped station):\n  mutate(dur = ifelse(dur == 0, 30, dur)) %>% \n  #calculate average speed between stops:\n  mutate(avg_kpH  =   stopDist_xprss / (dur/3600)) %>% \n  ungroup() \n\nstopDist_Dur_Speed_xprss %>% select(-Station)\n\n# A tibble: 122 × 14\n   order English     key     lon   lat stopD…¹ cumDist stopD…² service\n   <dbl> <chr>       <chr> <dbl> <dbl>   <dbl>   <dbl>   <dbl> <chr>  \n 1     1 Tokyo       dpt    140.  35.7    0       0       0    servic…\n 2     2 Ueno        arr    140.  35.7    3.78    3.78    3.78 servic…\n 3     2 Ueno        dpt    140.  35.7    0       0       0    servic…\n 4     3 Omiya       arr    140.  35.9   25.5    29.3    25.5  servic…\n 5     3 Omiya       dpt    140.  35.9    0       0       0    servic…\n 6     4 Kumagaya    arr    139.  36.1   33.4    62.7    33.4  servic…\n 7     4 Kumagaya    dpt    139.  36.1    0       0       0    servic…\n 8     5 Honjo-Wase… arr    139.  36.2   20.9    83.5    20.9  servic…\n 9     5 Honjo-Wase… dpt    139.  36.2    0       0       0    servic…\n10     6 Takasaki    arr    139.  36.3   18.9   102.     18.9  servic…\n# … with 112 more rows, 5 more variables: time <chr>,\n#   time_recode <dttm>, time_lag <dttm>, dur <dbl>, avg_kpH <dbl>,\n#   and abbreviated variable names ¹​stopDist, ²​stopDist_xprss\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n3a.\nCalculate average travel time between stops for all services\n\n\n#Join all stations (serviced_stn_long) with express service inter-stop distance, duration and speed:\njikoku_ttime_join <- serviced_stn_long %>% \n  select(1:3,key,lat,lon,service,contains('dist'),time,time_recode) %>% \n  #drop the stopDist and cumDist feilds which will otherwise become populated with NAs:\n  left_join(stopDist_Dur_Speed_xprss %>% select(-c(stopDist,cumDist))) %>% \n  #fill NA vals in avg_kpH (i.e., the average speed through non-stopping stations) \\\n  # with average speed for express services:\n  group_by(service) %>% arrange(service,order) %>% \n  fill(avg_kpH, .direction  =  'up') %>% \n  #calculate the duration between stations incl. non-stopping stations\n  mutate(dur_fill  =  stopDist/(avg_kpH/60)) %>% \n  #Division by stop time of 0 introduces NaN values. Replace these with original duration values:\n  mutate(dur_fill = ifelse(is.nan(dur_fill),duration(dur/60,'seconds'),dur_fill)) %>% \n  #convert estimated travel time to the the duration datatype\n  mutate(etime = duration(dur_fill,'minutes')) %>% ungroup()\n  \n\n#Convert duration to the period data type to allow summing with date times (validation only):\njikoku_ttime_join %>%\n  mutate(etime_period = as.period(etime)) %>%\n  #this yeilds the expected station stop clock times (except for stopping stations following non-stopping ones):\n  mutate(time_add = etime_period + time_lag)  %>% \n  select(-Station)\n\n# A tibble: 162 × 18\n   order English      key     lat   lon service  stopD…¹ cumDist time \n   <dbl> <chr>        <chr> <dbl> <dbl> <chr>      <dbl>   <dbl> <chr>\n 1     1 Tokyo        dpt    35.7  140. service…    0       0     13:…\n 2     2 Ueno         arr    35.7  140. service…    3.78    3.78 13:09\n 3     2 Ueno         dpt    35.7  140. service…    0       0    13:10\n 4     3 Omiya        arr    35.9  140. service…   25.5    29.3  13:28\n 5     3 Omiya        dpt    35.9  140. service…    0       0    13:29\n 6     4 Kumagaya     arr    36.1  139. service…   33.4    62.7  13:41\n 7     4 Kumagaya     dpt    36.1  139. service…    0       0    13:42\n 8     5 Honjo-Waseda arr    36.2  139. service…   20.9    83.5  13:50\n 9     5 Honjo-Waseda dpt    36.2  139. service…    0       0    13:51\n10     6 Takasaki     arr    36.3  139. service…   18.9   102.   14:00\n# … with 152 more rows, 9 more variables: time_recode <dttm>,\n#   stopDist_xprss <dbl>, time_lag <dttm>, dur <dbl>, avg_kpH <dbl>,\n#   dur_fill <dbl>, etime <Duration>, etime_period <Period>,\n#   time_add <dttm>, and abbreviated variable name ¹​stopDist\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n3b. Interpolate station\npass-thru times\nSo far we have validated the clock times based on summing the\ninter-stop duration and the previous row time data. This does not give\nthe clock time for the skipped stations, however, because there is not\nyet an estimated duration between non-stopping stations. In this\nsituation, the table must be iteratively updated as the time calculation\non row n depends on the presence of data in row n-1. Amazingly, as\ndetailed here,\nthe accumulate() function in the tidyverse easily\nfacilitates the requisite iterative row-filling:\n\n\njikoku_fillin_long <- jikoku_ttime_join %>% \n  mutate(etime_period = as.period(etime)) %>% \n  mutate(time_fill = time_recode) %>% \n  #this creates a vector from each column in the data frame - something like as.list()\n  vctrs::vec_chop() %>% \n  #accumulate function updates the new_row based on summing etime_period and time_fill if and only if the new row contains NA:\n  accumulate(function(prev_row,  new_row){\n    if(is.na(new_row$time_fill)){\n      new_row$time_fill = (prev_row$time_fill + new_row$etime_period)\n    }else{\n      new_row$time_fill = new_row$time_fill\n    }\n    new_row }) %>% bind_rows() \n\n#NB drop the Station column of Japanese characters which can't be rendered by this theme:\njikoku_fillin_long %>% select(-Station) \n\n# A tibble: 162 × 18\n   order English      key     lat   lon service  stopD…¹ cumDist time \n   <dbl> <chr>        <chr> <dbl> <dbl> <chr>      <dbl>   <dbl> <chr>\n 1     1 Tokyo        dpt    35.7  140. service…    0       0     13:…\n 2     2 Ueno         arr    35.7  140. service…    3.78    3.78 13:09\n 3     2 Ueno         dpt    35.7  140. service…    0       0    13:10\n 4     3 Omiya        arr    35.9  140. service…   25.5    29.3  13:28\n 5     3 Omiya        dpt    35.9  140. service…    0       0    13:29\n 6     4 Kumagaya     arr    36.1  139. service…   33.4    62.7  13:41\n 7     4 Kumagaya     dpt    36.1  139. service…    0       0    13:42\n 8     5 Honjo-Waseda arr    36.2  139. service…   20.9    83.5  13:50\n 9     5 Honjo-Waseda dpt    36.2  139. service…    0       0    13:51\n10     6 Takasaki     arr    36.3  139. service…   18.9   102.   14:00\n# … with 152 more rows, 9 more variables: time_recode <dttm>,\n#   stopDist_xprss <dbl>, time_lag <dttm>, dur <dbl>, avg_kpH <dbl>,\n#   dur_fill <dbl>, etime <Duration>, etime_period <Period>,\n#   time_fill <dttm>, and abbreviated variable name ¹​stopDist\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\nPLOT!\nFinally we have a complete data set for plotting!\nTo make a clean time for display we will round the time_fill to 10\nminute intervals:\n\n\njikoku_fillin_long_tm <- jikoku_fillin_long %>% arrange(time_fill) %>% \n  rename(Service=service) %>% \n  mutate(time_round = round_date(time_fill,'10 minutes'),\n         #strip date from round time column:\n         time_round = paste(sprintf(\"%02d\",   hour(time_round)), \n                            sprintf(\"%02d\", minute(time_round)), sep=\":\"))\n\n\n\n\nhokuriku_map_final <- hokuriku_map_elev + \n  ggnewscale::new_scale_colour() +\n  geom_emoji(data = jikoku_fillin_long_tm , \n             aes(x=lon, y=lat, group=Service), cex=0.05, \n             #emoji='1f685') +  \n             emoji = '1f686') +\n  #overlay a coloured square on each emoji to denote the service:\n  geom_point(data = jikoku_fillin_long_tm , \n             aes(x=lon, y=lat, group=Service, col=Service), \n             #pch=15, size=0.9, \n             pch=19, size=0.9, \n             show.legend = T) +\n  #add a clock timer by extracting time from the datetime data: \n  geom_text(data = jikoku_fillin_long_tm , \n            aes(x=139.5, y=37.25, group=1, \n            label = paste0('JR Hokuriku Line\\nTime: ', time_round), hjust='left'),\n          cex=3, check_overlap = T) +\n  theme(legend.position = 'bottom') \n\n\nhokuriku_map_final  \n\nanimate(hokuriku_map_final +  transition_reveal(time_fill ) + ease_aes('cubic-in-out') ,\n        height = 4, width = 6, units = \"in\", res = 300, \n        duration = 35 , fps=10)\n\n\nWhich produces the gif at the top of this post!\n\n\n\nTo save the animation as a gif:\n\n\ngganimate::anim_save('project_charts/hokuriku_anim_elev.gif')        \n\n\n… only 9 shinkansen lines to go!\n\nhttps://tinyurl.com/shinkansenmap\n\n\n\n",
    "preview": "posts/shinkansen/shinkansen_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2022-08-08T23:10:39+10:00",
    "input_file": {}
  }
]
